{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow101-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGQANEgW3N2vjJ5eiD8Arw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datasigntist/tensorflowCodes/blob/master/tensorflow101_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCOgXs-eKt_",
        "colab_type": "code",
        "outputId": "d0615a51-c040-451a-97aa-f2a67d9e7204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "import functools \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import pandas as pd\n",
        "\n",
        "LABEL_COLUMN = \"Survived\"\n",
        "\n",
        "TRAIN_DATA_URL=  \"https://raw.githubusercontent.com/datasigntist/datasetsForTraining/master/train.csv\" \n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL) \n",
        "\n",
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=5, # Artificially small to make examples easier to show.\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True, \n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
        "\n",
        "titanicDataSet = get_dataset(train_file_path)\n",
        "\n",
        "class extractFeaturesFromName(object):\n",
        "  def __call__(self, features, labels):\n",
        "    namesOfPassengers = features['Name']\n",
        "    lastName_feature = tf.map_fn(lambda feat: tf.strings.strip(tf.strings.split(feat,\",\")[0]) ,namesOfPassengers)\n",
        "    firstName_feature = tf.map_fn(lambda feat: tf.strings.strip(tf.strings.split(tf.strings.strip(tf.strings.split(feat,\",\")[1]),\".\")[1]) ,namesOfPassengers)\n",
        "    title_feature  = tf.map_fn(lambda feat: tf.strings.strip(tf.strings.split(tf.strings.strip(tf.strings.split(feat,\",\")[1]),\".\")[0]) ,namesOfPassengers)\n",
        "    features['last_name'] = lastName_feature\n",
        "    features['first_name'] = firstName_feature\n",
        "    features['title_feature'] = title_feature\n",
        "    return features, labels\n",
        "\n",
        "class replaceMissingValuesInAge(object):\n",
        "  def __init__(self, meanAge):\n",
        "    self.meanAge = meanAge\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    ageOfPassengers = features['Age']\n",
        "    ageOfPassengers = tf.map_fn(lambda age: (age if age>0 else self.meanAge) ,ageOfPassengers)\n",
        "    features['Age'] = ageOfPassengers\n",
        "    return features, labels\n",
        "\n",
        "class PackNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "desc = pd.read_csv(train_file_path)[\"Age\"].describe()\n",
        "MEAN = np.float32(np.array(desc.T['mean']))\n",
        "\n",
        "NUMERIC_FEATURES = ['Age','SibSp','Parch']\n",
        "\n",
        "CATEGORIES = {\n",
        "    'Sex': ['male', 'female']\n",
        "}\n",
        "\n",
        "newtitanicDataSet = titanicDataSet.map(extractFeaturesFromName())\n",
        "newtitanicDataSet = newtitanicDataSet.map(replaceMissingValuesInAge(MEAN))\n",
        "newtitanicDataSet = newtitanicDataSet.map(PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=None, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]\n",
        "\n",
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
        "\n",
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "train_data = newtitanicDataSet.shuffle(500)\n",
        "model.fit(train_data, epochs=20)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(train_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))\n",
        "\n",
        "predictions = model.predict(train_data)\n",
        "\n",
        "# Show some results\n",
        "for prediction, survived in zip(predictions[:10], list(train_data)[0][1][:10]):\n",
        "  prediction = tf.sigmoid(prediction).numpy()\n",
        "  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"SURVIVED\" if bool(survived) else \"DIED\"))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.6409\n",
            "Epoch 2/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5561 - accuracy: 0.7464\n",
            "Epoch 3/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7666\n",
            "Epoch 4/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7935\n",
            "Epoch 5/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7980\n",
            "Epoch 6/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7890\n",
            "Epoch 7/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8002\n",
            "Epoch 8/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8013\n",
            "Epoch 9/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8013\n",
            "Epoch 10/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.7946\n",
            "Epoch 11/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8103\n",
            "Epoch 12/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8002\n",
            "Epoch 13/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7980\n",
            "Epoch 14/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.8137\n",
            "Epoch 15/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8148\n",
            "Epoch 16/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8114\n",
            "Epoch 17/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.8047\n",
            "Epoch 18/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8047\n",
            "Epoch 19/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8058\n",
            "Epoch 20/20\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.8058\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.8159\n",
            "\n",
            "\n",
            "Test Loss 0.45728999376296997, Test Accuracy 0.8159371614456177\n",
            "Predicted survival: 17.49%  | Actual outcome:  DIED\n",
            "Predicted survival: 18.77%  | Actual outcome:  DIED\n",
            "Predicted survival: 18.87%  | Actual outcome:  DIED\n",
            "Predicted survival: 18.91%  | Actual outcome:  DIED\n",
            "Predicted survival: 79.04%  | Actual outcome:  SURVIVED\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}